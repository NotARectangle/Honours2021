{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "interact.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1a4Z7zXakf8czML5Us-Np15onSwmY5cjA",
      "authorship_tag": "ABX9TyPt2ys5jMysDpkr2pbhVnhp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NotARectangle/Honours2021/blob/main/interact.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMZDQHn8HDe0"
      },
      "source": [
        "From interact.py file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBGKJ1vlHqM6",
        "outputId": "6926c738-732e-4c5e-e37c-039da9363f9b"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/d5/f4157a376b8a79489a76ce6cfe147f4f3be1e029b7144fa7b8432e8acb26/transformers-4.4.2-py3-none-any.whl (2.0MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0MB 5.6MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 47.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 37.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=f5a15472b1ed78a38164c36dee66aa9bf642b82c1f330d0aff51ed134c4381f7\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.4.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZJIl3X5Hov1"
      },
      "source": [
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, top_k_top_p_filtering\n",
        "import re\n",
        "from torch.nn import functional as F"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UA-B37xGxHl",
        "outputId": "153f7d2d-2dc9-4cad-cd0f-9d71d301481b"
      },
      "source": [
        "modelPath = \"drive/MyDrive/Honours2021/TNGv3\"\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(modelPath)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(modelPath)\n",
        "\n",
        "input_str = \"PICARD: I am Captain Picard, commanding the Enterprise. PICARD: You will agree, Data, that Starfleet's orders are difficult? DATA: Difficult? Simply solve the mystery of Farpoint Station.\"\n",
        "\n",
        "personaID = \"<bos> TROI: I am Troi.\"\n",
        "persona2 = \"RIKER: I am Commander William T Riker.\"\n",
        "\n",
        "\n",
        "def generate_output(history):\n",
        "    sequence = history + \" TROI:\"\n",
        "\n",
        "    for i in range(150):\n",
        "\n",
        "        input_ids = tokenizer.encode(sequence, return_tensors='pt')\n",
        "\n",
        "        # get logits of last hidden state\n",
        "        next_token_logits = model(input_ids).logits[:, -1, :]\n",
        "\n",
        "        # filter\n",
        "        filtered_next_token_logits = top_k_top_p_filtering(next_token_logits, top_k=50, top_p=0.95)\n",
        "\n",
        "        # sample\n",
        "        probs = F.softmax(filtered_next_token_logits, dim=-1)\n",
        "        next_token = torch.multinomial(probs, num_samples=1)\n",
        "        n_token = next_token.tolist()[0][0]\n",
        "        bos_token_id = tokenizer.bos_token_id\n",
        "        if n_token == tokenizer.pad_token_id or n_token == bos_token_id:\n",
        "            continue\n",
        "\n",
        "        generated = torch.cat([input_ids, next_token], dim=-1)\n",
        "\n",
        "        resulting_string = tokenizer.decode(generated.tolist()[0])\n",
        "\n",
        "        #last_token = tokenizer.decode(next_token[0])\n",
        "\n",
        "        sequence = resulting_string\n",
        "        if sequence.rindex(\" \") > sequence.rindex(\".\"):\n",
        "          last_word = sequence[sequence.rindex(\" \") + 1:]\n",
        "        else:\n",
        "          last_word = sequence[sequence.rindex(\".\") + 1:]\n",
        "  \n",
        "        if re.match(\"([A-Z]+ ?[A-Z]+ ?:)|[A-Z]:\", last_word) or last_word in tokenizer.eos_token:\n",
        "            print(\"matched\")\n",
        "            resulting_string = resulting_string[: sequence.rindex(last_word)]\n",
        "            break;\n",
        "\n",
        "    #just generated outcome\n",
        "    resulting_string = resulting_string[resulting_string.rindex(\"TROI:\"):]\n",
        "    return resulting_string\n",
        "\n",
        "\n",
        "#give me for examples with same string\n",
        "#for i in range(4):\n",
        "#    generate_output(input_str)\n",
        "\n",
        "#start chat.\n",
        "history = [personaID, persona2]\n",
        "\n",
        "for i in range(5):\n",
        "\n",
        "    userinput = input(\"RIKER: \")\n",
        "\n",
        "    history.append(\"RIKER: \" + userinput)\n",
        "    context = \" \".join(history)\n",
        "    output = generate_output(context)\n",
        "    history.append(output)\n",
        "    print(output)\n",
        "    #labels we can do labels of pad"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RIKER: Good day Dianna.\n",
            "matched\n",
            "TROI: I'm Deanna Troi, Ship's Counsellor.\n",
            "RIKER: What do you think about the enterprice.\n",
            "matched\n",
            "TROI: I'm curious.\n",
            "RIKER: What is your name?\n",
            "matched\n",
            "TROI: I'm William Riker.\n",
            "RIKER: no. I am William Riker.\n",
            "matched\n",
            "TROI: Is there anything you'd like me to say? \n",
            "RIKER: Are you Dianna Troi?\n",
            "matched\n",
            "TROI: How long ago was it? \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}