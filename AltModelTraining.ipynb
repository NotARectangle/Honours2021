{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AltModelTraining.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1tbwxefX77V-doyou5J0LN6JH_4uY7I3c",
      "authorship_tag": "ABX9TyMwosfTIwvxfzW6+822BRbM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NotARectangle/Honours2021/blob/main/AltModelTraining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1diB7GmcXRq",
        "outputId": "1a56def2-9395-4be7-8a44-9ef25baa35e7"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.44)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_g805mkccpRr",
        "outputId": "302f1e85-4398-4f03-ff7a-37d14325be00"
      },
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooWHAw8tcQOT",
        "outputId": "e4a22b1d-9875-40f2-9d3a-bc3b8ad5f3c0"
      },
      "source": [
        "#import torch\n",
        "\n",
        "from transformers import GPT2Tokenizer, GPT2DoubleHeadsModel\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2DoubleHeadsModel.from_pretrained('gpt2')\n",
        "\n",
        "tokenizer.add_special_tokens({\"bos_token\": \"<bos>\", \"eos_token\": \"<eos>\", \"pad_token\": \"<pad>\"})\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of GPT2DoubleHeadsModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias', 'lm_head.weight', 'multiple_choice_head.summary.weight', 'multiple_choice_head.summary.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(50260, 768)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKVeTaBr6nz0"
      },
      "source": [
        "Train on Textfile."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-BZ5HiRmqNf"
      },
      "source": [
        "extract data from json"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSJuO5oxmsy7"
      },
      "source": [
        "import json\n",
        "import re\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data = json.load(open('drive/MyDrive/Honours2021/all_series_lines.json', 'r'))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APqD7fszoImL"
      },
      "source": [
        "\n",
        "script = {}\n",
        "\n",
        "s_name = \"TNG\"\n",
        "characters = [\"PICARD\", \"TROI\", \"DATA\", \"RIKER\"]\n",
        "script = {\"PICARD\":[], \"TROI\":[], \"DATA\": [], \"RIKER\": []}\n",
        "data = data[s_name]\n",
        "episodes = data.keys()\n",
        "for c in characters:\n",
        "  for ep in episodes:\n",
        "    for line in data[ep][c]:\n",
        "      #add bos token, persona, line and then eos token at end.\n",
        "      script[c].append(tokenizer.bos_token + \" \" + c + \": \" + line + \" \" + tokenizer.eos_token)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLQcj5JS3aNn"
      },
      "source": [
        "def build_text_files(data_json, dest_path):\n",
        "  f = open(dest_path, \"w\", encoding=\"utf-8\")\n",
        "  data = \"\"\n",
        "  for block in data_json:\n",
        "    for texts in block:\n",
        "      summary = str(texts).strip()\n",
        "      data += summary + \" \"\n",
        "  print(str(len(data)))\n",
        "  f.write(data)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STDD-DRl38ML",
        "outputId": "167761e3-f3d5-4b88-a637-98cefbc26ccb"
      },
      "source": [
        "train = []\n",
        "test = []\n",
        "for c in characters:\n",
        "  c_train, c_test = train_test_split(script[c], test_size=0.20)\n",
        "  train.append(c_train)\n",
        "  test.append(c_test)\n",
        "\n",
        "build_text_files(train,'train_dataset.txt')\n",
        "build_text_files(test,'test_dataset.txt')\n",
        "\n",
        "print(\"Train dataset length: \"+str(len(train)))\n",
        "print(\"Test dataset length: \"+ str(len(test)))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2015479\n",
            "506471\n",
            "Train dataset length: 4\n",
            "Test dataset length: 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiiu3K8L4jHz"
      },
      "source": [
        "Training and datasetBuilder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VrwzxiR4mmB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d3d470a-b145-472f-8797-b51e717ce657"
      },
      "source": [
        "from transformers import GPT2Tokenizer, GPT2DoubleHeadsModel\n",
        "\n",
        "train_path = 'train_dataset.txt'\n",
        "test_path = 'test_dataset.txt'\n",
        "model_path = \"drive/MyDrive/Honours2021/TNGv2\"\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
        "model = GPT2DoubleHeadsModel.from_pretrained(model_path)\n",
        "\n",
        "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
        "\n",
        "\n",
        "def load_dataset(train_path, test_path, tokenizer):\n",
        "    train_dataset = TextDataset(\n",
        "        tokenizer=tokenizer,\n",
        "        file_path=train_path,\n",
        "        block_size=128)\n",
        "\n",
        "    test_dataset = TextDataset(\n",
        "        tokenizer=tokenizer,\n",
        "        file_path=test_path,\n",
        "        block_size=128)\n",
        "\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer, mlm=False,\n",
        "    )\n",
        "    return train_dataset, test_dataset, data_collator\n",
        "\n",
        "train_dataset, test_dataset, data_collator = load_dataset(train_path, test_path, tokenizer)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:58: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_mlm.py\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t364JIUw44H_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "f497bb5e-e95a-4c81-bced-df65bd0dc66c"
      },
      "source": [
        "\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
        "    num_train_epochs=3, # number of training epochs\n",
        "    per_device_train_batch_size=32, # batch size for training\n",
        "    per_device_eval_batch_size=64,  # batch size for evaluation\n",
        "    eval_steps = 400, # Number of update steps between two evaluations.\n",
        "    save_steps=800, # after # steps model is saved \n",
        "    warmup_steps=500,# number of warmup steps for learning rate scheduler\n",
        "    prediction_loss_only=True,\n",
        ")\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,            # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=test_dataset             # evaluation dataset\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "trainer.save_model(\"./TNG\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='288' max='288' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [288/288 05:38, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gOuMGKj8PVL"
      },
      "source": [
        "test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDeRj1HN8Qom",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7341fb2-9064-4ab0-c455-64e406b42e27"
      },
      "source": [
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "makeItSo = pipeline('text-generation',model='./TNG', tokenizer=tokenizer,config={'max_length':1200})"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at ./TNG were not used when initializing GPT2LMHeadModel: ['multiple_choice_head.summary.weight', 'multiple_choice_head.summary.bias']\n",
            "- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AuEsGim8fxg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52127583-f312-4eb5-f51f-7b59d70879a8"
      },
      "source": [
        "makeItSo(\"PICARD: make it so.\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': \"PICARD: make it so.It is now and never was what it is. I cannot help thinking of her as one of my own, herself only.I think we should all have an idea. Let's bring the crew together because they\"}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9J-C_0a8EY7",
        "outputId": "3a375df2-d18a-4847-d67a-36b20e42579f"
      },
      "source": [
        "makeItSo(\"PICARD: You will agree, Data, that Starfleet's orders are difficult? DATA: Difficult? Simply solve the mystery of Farpoint Station. PICARD:\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': \"PICARD: You will agree, Data, that Starfleet's orders are difficult? DATA: Difficult? Simply solve the mystery of Farpoint Station. PICARD:Ensign.We'll send our own probe on that mission.Let's hope they find the Rom\"}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLzC7UuV8uG6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "217296e0-496d-48cc-dfd6-e93dda2bd6d2"
      },
      "source": [
        "makeItSo(\"PICARD: Would you object to your captain. RIKER:\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': \"PICARD: Would you object to your captain. RIKER:Aye, sir.And I know why, sir.If it is, we can't afford it.That does not appear to be your intent, Ambassador.I've not listened\"}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FgGioZo9DTV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c87e8fa-250c-4a39-8163-b2f800602a28"
      },
      "source": [
        "makeItSo(\"DATA: Can I ask you a question? PICARD: \")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'DATA: Can I ask you a question? PICARD: Yes, of course.Yes, Number One.You?No. Nothing, no, no. He thinks they are all dead. And then he gets distracted again, and they disappear.'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mG3n9dVHAM5v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf119c28-1d05-4a2c-adbe-f28d716e928e"
      },
      "source": [
        "makeItSo(\"Riker: Can I ask you a question? TROI: \")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': \"Riker: Can I ask you a question? TROI: If you have any doubts, don't worry. This is Commander Riker.Doctor, Doctor. Please tell himYou know who you are.What are we to find?\"}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBlb04Jr8tdy",
        "outputId": "004c5199-526e-49c6-86cc-8999bd903ec5"
      },
      "source": [
        "makeItSo(\"TROI:\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': \"TROI:, are you?How long have you been away?Will I be home soon?Riker, the Romulan warships are in formation.All right, I'll handle this. Let's go get the hell out of\"}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5GhawpTA2Gh"
      },
      "source": [
        "trainer.save_model(\"drive/MyDrive/Honours2021/TNGALTvs2\")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssXvQWkHn9h8",
        "outputId": "2cea0a3c-ecf2-437d-f0fe-73a1a79415b1"
      },
      "source": [
        "tokenizer.save_pretrained(\"drive/MyDrive/Honours2021/TNGALTvs2\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('drive/MyDrive/Honours2021/TNGALTvs2/tokenizer_config.json',\n",
              " 'drive/MyDrive/Honours2021/TNGALTvs2/special_tokens_map.json',\n",
              " 'drive/MyDrive/Honours2021/TNGALTvs2/vocab.json',\n",
              " 'drive/MyDrive/Honours2021/TNGALTvs2/merges.txt',\n",
              " 'drive/MyDrive/Honours2021/TNGALTvs2/added_tokens.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phFE-O-VlvG8",
        "outputId": "e1d4beff-eefc-4236-f720-fc963082e474"
      },
      "source": [
        "\n",
        "from transformers import pipeline, GPT2Tokenizer\n",
        "\n",
        "modelPath = \"drive/MyDrive/Honours2021/TNGALTvs2\"\n",
        "makeItSo = pipeline('text-generation',model=modelPath, tokenizer=modelPath,config={'max_length':1200})"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at drive/MyDrive/Honours2021/TNGALTvs2 were not used when initializing GPT2LMHeadModel: ['multiple_choice_head.summary.weight', 'multiple_choice_head.summary.bias']\n",
            "- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BzG0BPnr-EB",
        "outputId": "9b7e7209-4aea-4f7b-9360-96baa32fca23"
      },
      "source": [
        "makeItSo(\"USER: What do you think of this? PICARD:\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': \"USER: What do you think of this? PICARD:Why?Where are we? Our minds are on a mission to findthe truth.Mister Data, let's get right on it.Well done.Mister Worf\"}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9tpeRTUsK64",
        "outputId": "8f546400-27a0-4f81-a9a6-4093aabd46dd"
      },
      "source": [
        "makeItSo(\"RIKER: What do you think of this sir? PICARD:\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': \"RIKER: What do you think of this sir? PICARD:That does not stop me.He's getting into trouble. I have no choice but to help him.Mister La Forge, send an away team to theBridge.Thank you\"}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    }
  ]
}